Open-Source Agent Creation Platforms

Below we survey several open-source projects that can serve as lightweight, extensible platforms for building AI agents. We focus on repositories with permissive licenses (MIT, Apache 2.0, etc.) and evaluate their features against the requested criteria: user account management, Amazon Bedrock/Claude integration, document ingestion & RAG support, PowerPoint generation capabilities, Docker deployability with persistent storage, and visual workflow design. Key pros, cons, community activity, and extensibility notes are provided for each. Citations are included for verification.

Comparison Overview

Project	Language/Stack	License	Bedrock (Claude) Support	Doc Ingestion & RAG	User Accounts	Visual Workflow

**AWS Bedrock Chat (BrChat)**	Python (FastAPI) + AWS stack	MIT-0	Yes (native Bedrock API)	Yes (Knowledge Base RAG)	Yes (Cognito auth)	No (text-based config)
Bedrock Engineer	TypeScript/Electron app	MIT-0	Yes (built-in Bedrock tools)	Partial (uses Bedrock Knowledge base)	No (local app)	No (UI is app interface)
CrewAI Framework	Python	MIT	Yes (via SDK; Bedrock integration example)	Possible (via tools/vector DB)	No (library only)	No (code/flows definition)
Langflow	Python + React-flow UI	MIT	Yes (supports Amazon Bedrock)	Yes (via LangChain loaders/vector DB)	No (OSS lacks multi-user)	Yes (drag-and-drop builder)
Flowise	Node.js + React	Apache 2.0	Indirect (can call APIs; no native Bedrock)	Yes (nodes for PDF, DB, etc.)	No (single-user app)	Yes (drag-and-drop UI)
SuperAGI	Python + TypeScript UI	MIT	Not out-of-box (uses OpenAI; extensible to Bedrock)	Yes (toolkits: File, “Knowledge Search”)	Partial (local GUI; no multi-tenant)	Limited (prebuilt workflows)
**TypedAI (Nous)**	TypeScript (Node + Svelte)	MIT	Not native (uses OpenAI/Anthropic APIs)	Limited (focused on code agents)	No (single-user dev tool)	No (Web UI but not flow designer)


Table: High-level comparison of selected projects. ✅ = supported; ❌ = not supported out-of-the-box; “Partial” indicates possible with customization.

Below we discuss each project in detail, highlighting how they meet or miss the desired features, along with pros, cons, and community status.

AWS Bedrock Chat (BrChat)

Description: Bedrock Chat is an AWS-provided sample platform for building chatbots and agents with Amazon Bedrock. It’s a full-stack web application (FastAPI backend, React frontend) deployable on AWS infrastructure. BrChat supports multi-tenant user accounts (managed via Amazon Cognito) and project-specific bots. It natively integrates Bedrock APIs to access models like Anthropic Claude (Claude 3 Opus) and others via a unified interface. BrChat also includes Retrieval-Augmented Generation (RAG) capabilities through Amazon Bedrock Knowledge Bases: users can upload documents (PDFs, etc.) into a knowledge base, and the bot will retrieve relevant info via vector search (OpenSearch Serverless) during conversations. This allows building custom knowledge bots per project. Basic agent task automation is supported through an “Agent” feature that lets bots use tools or multi-step reasoning.

Key Features:

User Accounts & Projects: Yes – uses Cognito for sign-up/login and can restrict bot creation to authorized users. Multi-user bot sharing is supported via a “bot store” marketplace.

Amazon Bedrock Integration: Native – the platform is built specifically around Bedrock (no external LLM calls needed). You can choose any Bedrock model (Claude, Titan, etc.) for your bots. It even supports Bedrock Agent tools via AgentCore (in newer AWS releases).

Document Ingestion & RAG: Yes – integrates with Bedrock Knowledge Base to ingest PDFs, websites, etc. and do retrieval QA. Multi-turn conversation memory and multi-tenant knowledge bases are handled server-side.

Insight Generation & PPTs: Partial – It generates textual answers/insights from files, but no built-in PowerPoint output. (One could extend it by adding a new “tool” to format output as PPTX, but not available by default).

Deployment & Persistence: Deployed via AWS CDK script (serverless). Data is stored in AWS services: DynamoDB (chat history), S3 (uploads), etc., fulfilling persistent storage needs. It’s not a single Docker container, but you can spin it up easily in AWS CloudShell. (Running outside AWS would be non-trivial due to managed service dependencies).

Visual Builder: No drag-drop editor – bot configuration is done via forms (instructions, enabling knowledge bases, choosing tools) in the UI, but not a Node-RED style flow editor.


Pros: Comprehensive end-to-end solution for Bedrock – covers auth, multi-language UI, knowledge bases, and basic agent tools. Minimal setup if you have AWS access; scales using AWS serverless services. Good for enterprise AWS environments (secure, integrates with AWS logging/monitoring). **Permissive license (MIT-0)**.

Cons: AWS-centric and relatively heavy to deploy outside AWS. Lacks certain features like a graphical workflow designer or advanced autonomous agents beyond what's provided. Not focused on slide generation or complex multi-agent orchestration. Community-wise, it’s an AWS sample ( ~980 commits) and actively maintained by AWS but not a huge OSS community project (limited external contributors).

Use Case Fit: If you want a ready-made Bedrock-powered chat/RAG platform with user management, Bedrock Chat is a strong candidate. It meets most criteria except visual editing and PPT creation (those would require custom extension). Extensibility is moderate – you can add Lambda functions or new front-end components, but one must be familiar with AWS CDK and the deployed cloud architecture.

AWS Bedrock Engineer

Description: Bedrock Engineer is another AWS open-source project that targets a developer-centric AI agent experience. It’s essentially a desktop application (with a GUI) that leverages Bedrock under the hood to enable a “universal AI agent” for tasks like editing files, executing shell commands, searching the web, using a knowledge base, etc.. Think of it as an AI-powered IDE/assistant for developers, with multi-agent capabilities. It’s built with Electron/TypeScript and integrates tightly with Bedrock’s Agent tools. For example, it defines tools such as file system operations, web search via an API, Bedrock image generation, code execution, etc., which the agent can use autonomously. It also offers an Agent Directory (community-shared agents repository) and scheduling of agent tasks (cron-like background agents).

Key Features:

User Accounts: No – it’s a single-user desktop/native app (no multi-user web service). There is no username/password; it runs locally with your AWS credentials configured for Bedrock.

Amazon Bedrock Integration: Yes – all AI functionalities (LLM calls, image gen) use Bedrock. For instance, it can call Claude or Amazon’s Nova for image generation via Bedrock.

Document Ingestion & RAG: Partial – it can use Bedrock knowledge bases for RAG (so you could connect a knowledge base of documents) and the Agentic RAG is mentioned for connecting design system data sources. However, it’s not primarily a document Q&A tool; ingestion isn’t its main focus (more code and web tasks).

PPT Generation: Not specifically. The toolset includes code and web tools but nothing for generating PPT files.

Deployment: Distributed as a native app (PKG for Mac, etc.). No Docker – instead you “download and install” it on your machine. Data persistence is local (and through AWS for knowledge storage). It’s not a server with DB, so persistent storage is limited to local config files or what AWS services it uses.

Visual Workflow: No – the interface is a chat/command UI where you instruct agents. Customizing agents is via text prompts and toggling available tools, not a visual graph.


Pros: Rich set of agent tools and autonomy – great for power users who want an AI assistant that can, for example, write code or run command-line tasks with Bedrock’s safe environment. It even sandboxes code execution (using AWS Lambda or local WASM). MIT-0 licensed. Active (460+ stars) and developed by AWS folks, with community contributions possible.

Cons: Not a multi-user web platform. It’s more of a personal productivity agent than a collaborative service. Doesn’t handle user projects or have a web API. Lacks features like document upload UI or presentation generation – you’d have to script those on your own. Tightly bound to AWS services; if you need non-Bedrock model support or a lighter deployment, this might be overkill.

Use Case Fit: If your goal is to experiment with autonomous agents that can use tools (file I/O, web, etc.) in an AWS-friendly way, Bedrock Engineer is a strong base. But since it isn’t a server platform, it would require significant work to adapt into a web app with accounts. Consider it more as an inspiration or toolkit for agent capabilities (the repository could be mined for how to implement various tools with Bedrock) rather than the final hosted solution.

CrewAI (Open-Source Framework)

Description: CrewAI is a popular open-source Python framework for orchestrating multi-agent systems. Unlike the AWS samples, CrewAI is not a full app by itself but a library developers can use to build agent platforms. It emphasizes “agents working together” in two paradigms: Crews (teams of agents with different roles collaborating on tasks) and Flows (structured, event-driven workflows that can incorporate agents). CrewAI is known for being lightweight and fast (built from scratch, independent of LangChain) and highly extensible. It comes with a library of 100+ tools that agents can use (web search, code execution, vector DB queries, etc.). Notably, CrewAI has been integrated with Amazon Bedrock in real-world use cases – AWS published a blog showing how CrewAI’s framework can deploy multi-agent systems on Bedrock. CrewAI is MIT-licensed and has a very large community (44k stars on GitHub), indicating strong activity and support.

Key Features:

User Accounts/Projects: No built-in user management – CrewAI is a backend framework. Any account system would need to be added by the developer in a custom app that uses CrewAI.

Amazon Bedrock Support: Yes, in the sense that CrewAI can call any model via SDK or API. CrewAI abstracts model calls, and you can plug in Bedrock by using AWS SDK calls within it. Indeed, AWS’s blog notes that combining CrewAI with Bedrock Agent APIs enables powerful multi-agent deployments. (CrewAI itself supports many model providers via its config, so adding Bedrock would be straightforward).

Document Ingestion & RAG: Possible – CrewAI doesn’t provide a GUI for uploading docs, but you can use its tools to connect to vector databases or knowledge sources. For example, one could integrate LlamaIndex or Haystack within a CrewAI flow to handle document retrieval. Out of the box, CrewAI provides a memory module and you can use any embedding/vector store in code.

PPT Generation: No specific feature for this. You would have to incorporate a Python library like python-pptx and possibly create a custom tool or post-process step in a flow to generate slides from text.

Deployment: As a Python library, you can containerize your CrewAI app. It’s lightweight (no heavy external dependencies apart from LLM APIs). Persistent storage (for agent memory or state) is up to you – e.g., you can use a database or file system via tools. CrewAI itself doesn’t force a particular DB (some use SQLite or Dynamo or Postgres depending on integration).

Visual Workflow Builder: Not built-in. Workflows (Flows) are defined in Python code (or YAML) programmatically. There’s no drag-drop UI in the open-source framework. (CrewAI’s company offers a separate “Control Plane” enterprise UI, but that’s a hosted service).


Pros: Highly extensible and modular – ideal if you want a code-first approach to building a custom agent platform. It supports complex multi-agent interactions (e.g., agents delegating tasks to each other) and structured flows for conditional logic. CrewAI is designed for performance and control; you can intercept and customize every step. The community and support are strong (100k+ developers certified via their courses), and it’s actively maintained (1.9k commits). Permissive MIT license.

Cons: Not an out-of-the-box product – requires significant development to assemble a full platform (UI, auth, etc.). No built-in UI or storage means more initial setup for a complete solution. The learning curve can be moderate; you must design your agent workflows or “crew” configurations. If you specifically need a visual builder or non-coding interface, you’d have to integrate CrewAI with another tool (notably, Langflow has added support for CrewAI as a backend – see below).

Use Case Fit: If a complete solution doesn’t exist, CrewAI is one of the best base frameworks to build on for a custom agent platform. For example, one could build a web front-end (perhaps using FastAPI + React) that uses CrewAI under the hood to manage agents and calls Bedrock models. Gaps would include implementing the user account system and any UI (plus something for PPT generation). Given CrewAI’s popularity and active development, it’s a future-proof choice for underlying agent logic, with the trade-off of having to create the surrounding infrastructure.

Langflow (UI for LangChain & Agents)

Description: Langflow is an open-source low-code builder for AI applications – essentially a graphical interface to create and deploy LLM chains and agents. It started as a UI for LangChain, but has evolved into a broader platform. Langflow allows you to drag and drop components (LLM models, prompts, tools, vector stores, etc.) and connect them into a flowchart representing an agent’s logic or a RAG pipeline. It’s written in Python (FastAPI backend) with a React-flow front-end. Crucially, Langflow now supports all major LLM providers and vector DBs, including Amazon Bedrock models. That means you can configure a node in the flow to use Claude via Bedrock (with appropriate AWS credentials) – Bedrock is explicitly listed among its connectors. Langflow supports agents (via LangChain’s agent frameworks or its own logic) and can also act as an MCP (Model Context Protocol) server, allowing external control of context.

Key Features:

User Accounts: Not in the open-source version. Langflow OSS runs as a single-user web app (no login system by default). They do offer a hosted cloud service with accounts, but if self-hosting you’d need to put it behind an auth proxy or add an auth layer.

Amazon Bedrock Integration: Yes – built-in. The UI lets you select Amazon Bedrock and choose available models, making calls via AWS SDK. This covers access to Claude (Anthropic via Bedrock) as requested.

Document Ingestion & RAG: Yes – Langflow includes components for document loaders (PDFs, text, etc.) and vector stores (Pinecone, Weaviate, Chroma, OpenSearch, etc.). You can create a RAG flow: e.g., a PDF loader -> text splitter -> embedding model -> vector DB -> retrieval -> LLM answer. This is one of its core use cases (the Langflow docs highlight RAG pipelines).

Insight Generation & PPT Creation: Partially. Generating insights from docs (summaries, Q&A) is straightforward in Langflow. However, no native PPT generation node exists. You could incorporate a custom component using a library like python-pptx to output slides (the platform allows custom code in components). There’s also nothing stopping you from generating a textual outline or Markdown and then converting to slides externally. But out-of-the-box, it won’t directly spit out a .pptx file without extension.

Deployment & Persistence: Langflow is Docker-friendly – it provides a Dockerfile and can be deployed on various clouds (the docs mention AWS, Azure, GCP deploy guides). Flows can be saved in a database; by default it might use a local SQLite or in-memory store, but it can be configured for persistence. For production, you’d likely connect it to a Postgres database to store flow definitions and chat histories. The server itself is lightweight (just needs the LangChain and FastAPI stack plus whichever vector DB you use).

Visual Workflow Builder: Yes – this is Langflow’s highlight. A graphical drag-and-drop canvas to design agent workflows without coding. It’s intuitive for prototyping and explaining logic. The flows can be run in the UI or via an API endpoint, and you can iterate quickly.


Pros: Highly user-friendly interface for constructing complex chains/agents. Great support for RAG and various model providers (including open models and Bedrock). The project is active and has an emerging community (the GitHub fork was mentioned with 8.5k forks; the main project has many users – “thousands of developers” as per their site). MIT license – permissive for commercial use. It strikes a good balance between low-code design and extensibility – advanced users can inject Python code for custom logic where needed. Langflow also supports deploying flows as APIs, so one could integrate it with a custom front-end.

Cons: Lacks built-in multi-user management – every instance is essentially single-tenant. Security/auth would need to be configured at the server level. While visual building is powerful, debugging complex flows can get tricky (as with any low-code tool). Also, performance for very large flows or heavy concurrent usage might require scaling the backend (which you can do since it’s stateless aside from the DB). For PPT generation, you’d need to develop a bit on top. Community support is growing but not as massive as LangChain’s itself (though the Langflow team appears to be actively releasing new features – e.g., recent Bedrock and CrewAI integration as shown on their site).

Use Case Fit: Langflow covers many of the requested features directly: Bedrock/Claude access, document ingestion, RAG, Docker deploy, etc. The major gap is user accounts/projects – to address that, you could run Langflow behind an authenticating reverse proxy or modify it to support multi-user (not trivial, but possible by namespacing flows per user in the DB). Also, to create PPTs, you might integrate the AWS sample logic (see PPT Generation section below) as a custom tool in Langflow. Overall, if you want a quick-start platform and are willing to operate it for a single team or behind VPN, Langflow is a top choice. It’s an extensible low-code base that you could extend to a fuller product.

Flowise

Description: Flowise is an alternative low-code visual builder for LLM flows – similar concept to Langflow but implemented in Node.js/TypeScript. It provides a drag-and-drop interface (built on React Flow) to chain together prompts, models, and tools, and is powered by LangChainJS under the hood. Flowise has gained significant traction (over 21k GitHub stars) as a user-friendly way to create chatbots and AI assistants without coding. It’s often compared alongside Langflow; one key difference is the tech stack (Node vs Python). Flowise is Apache 2.0 licensed.

Key Features:

User Accounts: No built-in auth (like Langflow, it’s single-user by default). Typically, you run it locally or on a server and perhaps restrict access via network rules.

Amazon Bedrock Integration: Not out-of-the-box. Flowise focuses on OpenAI, HuggingFace models, etc., configurable via API keys. It does not yet have native support for Amazon Bedrock. However, because it allows custom nodes, one could write a node that calls Bedrock’s API (using AWS SDK) to get Claude responses. This would require development in TypeScript. The Flowise docs/community might have examples of calling other APIs. In summary, Bedrock is not plug-and-play in Flowise like it is in Langflow.

Document Ingestion & RAG: Yes – Flowise includes nodes for file input (PDF, text) and vector store operations, since LangChainJS supports those. You can set up an embedding model and vector DB in a flow and do retrieval Q&A. Many users employ Flowise to build chatbot QA on their documents.

PPT Generation: Not supported natively. You would need to create a custom component or have the flow call an external API to generate a PPT. Since Flowise can call REST APIs (through code nodes), one idea is to hook it up to an endpoint running the AWS PPT generator logic.

Deployment & Persistence: Flowise is designed for easy self-hosting. It has a Docker container configuration and can be deployed to cloud platforms (the docs mention support for AWS, Railway, etc.). It uses a lightweight SQLite database by default to save flows and chat history (or can be configured to use others). For production, switching to Postgres is advisable (the community has done so, given Node’s Prisma or TypeORM can be configured in code). It doesn’t require heavy resources; Node 18+ is needed.

Visual Workflow: Yes – very similar experience to Langflow. A palette of nodes (LLM, tools, input/output) that you connect visually to design the agent’s reasoning path.


Pros: No-code ease and fast iteration for building LLM apps. As an Apache-2.0 project, it’s friendly for enterprise use. It has a robust community (trending on GitHub, active Discord) and a growing library of third-party node contributions. Flowise is lightweight and runs entirely on your machine or server, giving you full control (no external dependencies aside from whichever LLM APIs you plug in). Good coverage of RAG use cases and integration with LangChain’s JavaScript ecosystem.

Cons: Lacks native Bedrock/Claude support, which is a significant gap for this use case – you’d have to implement that. Also lacks multi-user features natively. Since it’s Node-based, extending it requires TypeScript skills, which might be a con if your team is more Python-oriented. Another consideration: LangChainJS (which Flowise uses) historically lags a bit behind LangChain Python in features, though it’s improving. If your use of RAG/agents needs very cutting-edge features, ensure they exist in the JS version.

Use Case Fit: Flowise could be the base if you prefer a Node/TypeScript stack or find its interface more to your liking. It ticks many boxes (document Q&A, Docker deploy, persistent storage). To meet all criteria, you’d need to extend it: add a Bedrock connector (for Claude) and some user management wrapper. Given that Langflow now has Bedrock built-in, Flowise might be a second choice unless you specifically need a JS solution. Its permissive license and strong community are pluses. Extensibility: high – you can create custom nodes in the components module, meaning you could integrate things like calling the Bedrock Converse API or even generating PPT files via a Node library. These would be non-trivial but certainly feasible with moderate effort.

SuperAGI

Description: SuperAGI is a full-fledged autonomous AI agent framework and platform. It’s dubbed “dev-first,” aiming to let developers quickly build and deploy agents that can perform useful tasks autonomously. SuperAGI differs from Langflow/Flowise in that it comes with a ready-to-use web interface (dashboard) and more structured agent management out of the box. Under the hood it’s Python (FastAPI for backend, likely a front-end in Next.js or similar) and uses a database (PostgreSQL) to manage agents, tasks, and tool configurations. It’s MIT-licensed and has significant popularity (17k stars). SuperAGI supports running multiple agents concurrently, each with their own “toolkits” (predefined sets of tools/skills). It provides a GUI where you can provision agents, monitor their runs, and intervene via an “Action Console” (human in the loop). It also offers a marketplace of toolkits to extend agent capabilities (e.g., tools for search, file I/O, email, coding, etc.).

Key Features:

User Accounts & Multi-tenancy: Partial. SuperAGI’s open-source version does have a web UI with authentication, but it might be a single admin user or simple auth by config (this is a bit unclear in docs). It’s not designed as a multi-organization SaaS out of the box – more like a control panel for one deployment. However, it does separate “workspaces” or agent projects conceptually, which could map to user projects. If true multi-user support is needed, some adaptation might be required (the cloud version of SuperAGI likely has it).

Amazon Bedrock (Claude) Support: Not built-in. SuperAGI primarily supports OpenAI models and local models (and possibly Azure OpenAI) by default. It doesn’t currently list Bedrock integration. You would need to add a custom LLM provider in the code or use Bedrock via a custom “Toolkit” (for example, a toolkit that calls Bedrock’s API for generation). The architecture is pluggable (it has a Models interface and a Tools interface), so integrating Bedrock is doable. But as of now, if you install SuperAGI, you can’t just select Claude unless you route through the OpenAI-compatible API (Claude isn’t OpenAI API compatible, so that’s a challenge unless via Bedrock or Anthropic’s API).

Document Ingestion & RAG: Yes. SuperAGI has “Knowledge Search” and file handling toolkits. For example, it has a File Manager tool (to read/write files) and likely integrations to vector stores (multiple vector DBs are mentioned as supported). So an agent can be configured to use a knowledge vector DB and do RAG-style retrieval as one of its actions. What it doesn’t have is a slick UI to upload documents and automatically embed them – you might need to feed the data in via the CLI or ensure an agent’s tools are pointed to a folder of docs. Nonetheless, the plumbing for RAG is present (connectors to Pinecone, Weaviate, etc., as indicated in their marketing).

Insight Generation & PPT Creation: Insight generation (summaries, reports) can be done by agents. For PowerPoint specifically, no first-class support. You could integrate a Python library as a tool (similar to others). Indeed, SuperAGI’s extensibility via toolkits means you can add a “PPTGenerator” tool that takes some content and produces a .pptx – you’d implement it in Python and register it. The SuperAGI Medium article suggests it’s straightforward to add custom tools to extend functionality.

Deployment & Persistence: SuperAGI comes with Docker compose files for quick setup (including Postgres, etc.). It uses a relational DB to store agent configs, run logs, etc. – persistent by nature. Deploying is more involved than Langflow since multiple services are needed (API server, frontend, worker processes, DB, Redis for queues possibly). But their documentation provides steps, and being open-source, you can host it on any infrastructure. It’s heavier than Flowise/Langflow but still “lightweight” compared to huge enterprise platforms.

Visual Workflow Builder: Not exactly. You don’t visually draw the agent’s logic; instead you configure an agent by specifying its goal, tools available, constraints, etc., in the UI or YAML. SuperAGI does mention “Workflows – predefined steps using ReAct patterns”, which hints at templated reasoning chains, but there is no drag-drop editor. The UI is more about managing agent runs, viewing traces, and tweaking parameters, rather than drawing flow diagrams.


Pros: Comprehensive platform out-of-the-box – SuperAGI gives you a web UI, agent orchestration, memory storage, support for numerous tools, and monitoring, without having to assemble those pieces yourself. It’s one of the more “enterprise-ready” open frameworks (they emphasize security, scalability, etc., and have an ecosystem with mobile app, browser extension, etc.). Extensibility is a core focus: new tools (toolkits) can be added easily, and there’s a community marketplace for sharing them. Active community and ongoing development are positives – it’s a trending project and likely to keep improving. The MIT license is permissive.

Cons: Bedrock not supported by default – integration effort needed for Claude via Bedrock. Also, the system has many moving parts (which means a bit more devops effort to deploy and maintain). If your goal is a lightweight hackable platform, SuperAGI might feel a bit heavy: it brings its own way of doing things (ReAct agents, console control, etc.) which you’ll need to learn. Another con: the UI, while existing, is not a visual workflow builder, so designing complex RAG or conditional flows might require writing or modifying code in the backend. In terms of accounts, using it in a multi-user environment might require adding role-based access control on top.

Use Case Fit: If you want a pre-built foundation for an agent platform and are willing to customize it, SuperAGI is a strong candidate. It already satisfies persistent storage (PostgreSQL), multi-agent management, and an extensible tool interface. The gaps would be hooking in your desired LLM provider (Bedrock) and possibly creating a user-facing interface for uploading documents or initiating PPT generation (which could be done by wrapping SuperAGI’s API with a small front-end). It’s a good middle ground between building from scratch (CrewAI) and a simple demo app – much of the groundwork (task management, concurrency, UI) is done. For extensibility, the team provides guides (e.g., adding custom tools), so one could relatively quickly add a “Claude via Bedrock” tool or model connector and a “GeneratePowerPoint” tool to meet those specific needs.

TypedAI (Project “Nous”)

Description: TypedAI, formerly introduced as “Nous”, is a more obscure open-source project that targets TypeScript developers. It’s essentially a TypeScript AI agent platform with a web UI and support for autonomous coding agents and chatbots. It was presented on Hacker News as an “all-in-one” framework combining ideas from CrewAI, DevOps automations, and Langfuse tracing. The core is written in Node/TypeScript (hence “TypedAI”) and it features:

A web interface for chat and agent management.

A coding agent that can review code (integrating with GitLab/GitHub for PRs).

An autonomous agent that uses function calling to plan and execute tasks (with a pyodide sandbox to run Python code securely in-browser).

Tracing and database persistence of agent runs.


It’s MIT-licensed and has around 1.2k stars, indicating some interest. This platform is interesting if one prefers a JavaScript stack over Python. It includes some out-of-the-box use cases (DevOps helpers, etc.).

Key Features:

User Accounts: Likely no multi-user support out-of-box. It might have a simple login for the web UI, or perhaps it’s single-user mode when you run it. The HN thread did not emphasize multi-tenant aspects.

Amazon Bedrock (Claude) Support: Not explicitly. However, the repository structure showed a .claude folder, which suggests it may have integrated Anthropics Claude API usage (possibly via their direct API or a dev key). If not, it definitely works with OpenAI function calling. Bedrock is not mentioned, so you’d have to integrate that similarly to how you’d do in Flowise (i.e., by coding an API call to Bedrock’s endpoint).

Document Ingestion & RAG: Not a focus. It seems geared more toward code and web tasks. It’s likely not pre-integrated with vector DBs or file Q&A. You could extend it, but it’s not a built-in feature.

PPT Generation: No – not in scope for this project as released.

Deployment & Persistence: It uses Docker (the HN comments mention a docker-compose up to get it running easily). It has a database (likely PostgreSQL via Prisma) to store data. Since it’s TypeScript, one would deploy it like a typical Node web app (the code and compose files are provided).

Visual Workflow: No visual editor; interactions are through a UI but flows are predefined in code (TypeScript classes for agents and their behaviors).


Pros: For a TypeScript environment, TypedAI provides a lot: web UI, persistent storage, and a demonstration of complex autonomous agent logic with sandboxed execution. It’s lightweight in that you can run it with just Node and a DB. MIT license. Also, it shows innovation in using function calling in a model-agnostic way and combining multiple agent types in one framework.

Cons: Quite new/experimental – smaller community, fewer contributors. Documentation may be sparse beyond the code. Missing many high-level features (doc QA, Bedrock integration by default, etc.). Choosing this would likely require significant modifications to meet all criteria, and given its overlap with what Langflow/SuperAGI provide (but in a less proven form), one might only choose TypedAI if committed to a TypeScript codebase for everything.

Use Case Fit: If your team’s strength is JavaScript/TypeScript and you want an agent platform to hack on in that language, TypedAI (Nous) is worth looking at. It gives a starting point with accounts (maybe minimal), project structure, and some agent examples. However, to meet the full list of requirements, you’d have to implement Bedrock API calls (for Claude), add a document ingestion pipeline (perhaps via calling an external vector DB service), and integrate PPT generation logic. This is certainly doable but you’d be writing a lot of custom TS code. In contrast, Python-based solutions have more off-the-shelf components for these needs. So this is a niche option – a strong base only if avoiding Python is a priority.

Other Notable Mentions

Haystack (deepset) – An Apache-2.0 Python framework mainly for QA chatbots with RAG. It recently introduced an Agents API allowing tools and planning. It’s solid for document ingestion and uses PostgreSQL/Opensearch for storage. However, it doesn’t handle user accounts or Amazon Bedrock directly (you’d use OpenAI or local models). There is a UI (Haystack Playground) but it’s rudimentary. Could be a base for building a Bedrock RAG system if combined with AWS SDK calls.

Microsoft Semantic Kernel – An open-source SDK (MIT license) for creating AI “skills” and chaining prompts with planners. It supports C# and Python. Good for planning and has plugins for memory, but it’s more of a dev framework (no UI or user management). It could be used to compose workflows and even has example web apps, but integrating Bedrock would require using AWS SDK (SK doesn’t natively support Bedrock as of now).

AutoGen (Microsoft) – A research framework for multi-agent conversations (asynchronous messaging between ChatGPT-like agents). Useful if you need agents to converse with each other in roles (e.g., brainstormer and solver). But it’s research code (Python) and not a full platform (no auth, no persistence).

CAMEL Framework – A research-oriented multi-agent role-playing framework (Apache-2.0) focusing on scaling to many agents. It’s powerful for simulations and “societies” of agents, but again not a user-facing platform.

Pydantic AI – This is a smaller library (advertised on Reddit) that uses Pydantic models to structure LLM interactions. It provides a lightweight way to define agent behavior and validate outputs. If one wanted a minimal Python agent framework, this could be interesting. It doesn’t meet most of the high-level criteria (no UI, etc.), but it’s “simple and clean” to define agents in code. Perhaps useful for integrating into a larger system for output validation and type enforcement.

Portkey/Swarm/ADK – (From search results) There are new frameworks like OpenAI’s Swarm (lightweight multi-agent orchestration in Python, from OpenAI’s evals team) and Google’s Agent Development Kit (ADK). These are cutting-edge but very early-stage; they focus on how to structure agent reasoning and might not be stable or full-featured yet.

Presenton (Gamma Alternative) – For the specific task of generating slide decks, an open-source project called Presenton exists. It’s a local app that generates presentations with AI (likely using local models). It supports taking a PDF or doc and creating a PowerPoint (PPTX) or PDF slides output. This is more specialized, but if slide creation is a major requirement, one could incorporate ideas or even code from Presenton. Its approach ensures privacy (runs locally) and it likely uses a combination of an LLM for text and an image generator for slide visuals.


PPT Generation Solutions

No single agent platform above natively outputs PowerPoint files, but there are open-source examples of this capability:

**AWS Samples – “Generate your presentation with LLM”**: A small streamlit app (MIT-0 licensed) that takes a topic and uses Amazon Bedrock (Claude models via Bedrock’s Converse API with function calling) to create a PPTX file with slides. It uses python-pptx to build slides and Amazon Nova for image generation. This is a great reference implementation of PPT generation using Claude on Bedrock, fulfilling exactly the “generate new PowerPoint presentations from uploaded files or topics” need. It even shows slide thumbnails by integrating OpenOffice for preview. One could integrate this into a larger platform (for example, as a callable service or toolkit within SuperAGI or as a custom node in Langflow). The trade-off is that it’s a standalone demo (40 stars), not a multi-user system.

**PPTAgent (Research)** by Chinese Academy of Sciences: A research codebase that uses an agentic approach to generate slides by analyzing example decks and iteratively creating content. This is more of an academic solution, possibly less accessible but might have novel ideas (it’s on GitHub).

Other scripts/notebooks: e.g. llm-powerpoint – a simple Jupyter notebook demonstrating GPT-based PPT creation; slides_generator by AI-Forever – a framework for single-prompt PPT generation.

Gamma.ai Alternatives: Gamma is a SaaS for AI-generated presentations. Presenton (noted above) is one open alternative and there are others like slide-deck-ai and PowerPointer for Local LLMs. These might not integrate with Bedrock, but one could adapt them to use Claude via Bedrock as the text generator.


Integration Suggestion: To add PPT generation to any agent platform, the likely approach is:

1. Use an LLM (Claude via Bedrock) to generate a structured outline of slides (maybe using function calling to get JSON output of slides content and titles).


2. Feed that into a PPTX generator library (e.g., python-pptx in Python or pptxgenjs in Node) to produce the actual file.


3. If images are needed, call an image model (Bedrock’s stable diffusion or DALL-E API) for each slide background. This is exactly what the AWS sample does with Streamlit. Thus, incorporating that code into, say, a SuperAGI toolkit or a Langflow custom component would cover the “generate new PowerPoint presentations from input” requirement.



Conclusion & Recommendations

No single open-source project perfectly satisfies all the criteria; however, several come very close and can be combined or extended to achieve the goal:

Most Complete Base: AWS Bedrock Chat is the only one that natively has user accounts, Bedrock Claude support, RAG ingestion, Docker/AWS deployment – essentially everything except the visual builder and PPT output. If operating fully in AWS is acceptable, Bedrock Chat could be adopted and then extended. For example, one could integrate the PPT generation code (from the AWS Streamlit sample) as an additional “agent tool” or backend service that BrChat calls. The gap would be the lack of a visual workflow editor; however, BrChat’s existing UI might suffice for end users (less developer-focused).

Best Low-Code Option: Langflow stands out for ease of building and supporting Bedrock off-the-shelf. It handles document ingestion and can connect to persistent storage. To turn Langflow into a multi-user SaaS, you’d need to layer on authentication and per-user flow segregation. If that’s too much, running a single-user instance for each user (or just internally) might be an interim solution. PPT generation can be added to Langflow with a custom node that uses the Bedrock Claude model in a function-calling mode to output slide content and then calls a Python function to create the PPTX (this would require writing that node, but given Langflow’s design, it’s feasible).

Best Framework for Custom Dev: CrewAI (with possibly SuperAGI or Flowise components). For a truly extensible, code-first approach, using CrewAI for the agent logic and either building a simple UI or embedding it into a Flowise/Langflow UI could yield a powerful platform. For instance, one could have CrewAI handle multi-agent task orchestration behind the scenes while Langflow’s UI is used for designing flows or providing user interaction. CrewAI ensures future flexibility (new kinds of agents, complex workflows), at the cost of writing more custom code upfront.

SuperAGI as an Out-of-the-Box Platform: If accounts are not a hard requirement (or can be handled with a single admin), SuperAGI offers a quick start. You’d integrate Bedrock by adding a model provider in its config (the community might have plugins for AWS already), and add the PPT generation logic as a custom tool. The pros are you get a UI to manage agents and can leverage existing toolkits (for web search, file IO, etc.) immediately. The cons are you may need to strip out or adjust parts that assume OpenAI APIs and ensure everything runs with Bedrock instead.


Community & Maintenance: All the highlighted projects are permissively licensed and active to varying degrees:

CrewAI and SuperAGI have strong communities and are likely to continue evolving (CrewAI just hit v1.0 GA with lots of users).

Langflow/Flowise are in active use by many developers prototyping LLM apps, so they get frequent updates.

AWS’s samples (BrChat, Bedrock Engineer) will likely be updated as AWS improves Bedrock/Agents (they already updated BrChat to v3 with a bot marketplace recently, and support new Claude versions as they come).

You should check license details: all listed are MIT or Apache 2.0, so no strong copyleft to worry about. (AWS’s MIT-0 is basically MIT with no attribution requirement).


Gaps to Note Clearly: No solution is turnkey for “Claude via Bedrock + multi-user + RAG + PPT + visual builder”. You will need to do some integration work. Summarizing major gaps:

User management: add auth to Langflow/Flowise, or accept single-user mode, or use BrChat/SuperAGI which have some auth built-in.

Claude via Bedrock: ensure the chosen platform can call Bedrock’s API (Langflow yes, others require code changes).

PPT output: integrate a custom module (reuse AWS’s code if possible).

Visual builder: if important, lean towards Langflow/Flowise; if not, the AWS samples or SuperAGI’s simpler UI might suffice.


Finally, consider that if building an internal tool, you might combine approaches. For example, one could use Langflow to design the workflow, then export it and run it within a CrewAI or SuperAGI backend that handles scale and user management, with Bedrock providing the AI power, and leverage AWS’s PPT generator for the final mile. This hybrid approach would maximize use of open-source components while minimizing custom code.

Sources:

Amazon Bedrock Chat – AWS sample platform (MIT-0)

Amazon Bedrock Engineer – AWS agent desktop app (MIT-0)

CrewAI Framework – Multi-agent Python framework (MIT); AWS on CrewAI+Bedrock

Langflow – Low-code LangChain/agent builder (MIT)

Flowise – Visual LLM flow builder (Apache 2.0)

SuperAGI – Autonomous agent framework/platform (MIT)

TrafficGuard TypedAI (Nous) – TypeScript agent platform (MIT)

AWS Bedrock RAG Sample – Knowledge Base ingest and Q&A

AWS “Generate Presentation with LLM” – Bedrock Claude to PPT demo

Reddit discussion on agent frameworks (Pydantic AI mention) and HackerNews discussion on Nous.